{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import numpy as np\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from utils.model import predict, load_model, Metrics, save_model, lgb_booster_to_model, load_final_model, save_final_model, load_final_model\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pipeline import Pipeline, PipelineStep\n",
    "\n",
    "from steps.set_missings import set_missings, drop_fraud\n",
    "from utils.helpers import reduce_mem_usage\n",
    "from steps.load_data import load_train_data, load_test_data\n",
    "from steps.feature_selection import feature_selection\n",
    "from steps.dpi_features import generate_test_dpi_feature, generate_train_dpi_feature\n",
    "from steps.bnum_features import generate_test_bnum_feature, generate_train_bnum_feature\n",
    "from steps.fe_features import generate_test_fe_feature, generate_train_fe_feature\n",
    "\n",
    "\n",
    "def remove_abon_id(df):\n",
    "    return df.drop(\"abon_id\", axis=1)\n",
    "\n",
    "\n",
    "transform_train_pipeline = Pipeline(\n",
    "    \"TRANSFORM_TRAIN\",\n",
    "    [\n",
    "        # Load data\n",
    "        PipelineStep(load_train_data),\n",
    "        PipelineStep(set_missings),\n",
    "        PipelineStep(reduce_mem_usage),\n",
    "        # New features\n",
    "        PipelineStep(generate_train_fe_feature),\n",
    "        PipelineStep(generate_train_dpi_feature),\n",
    "        PipelineStep(generate_train_bnum_feature),\n",
    "        PipelineStep(drop_fraud),\n",
    "        # Feature selection\n",
    "        PipelineStep(remove_abon_id),\n",
    "        PipelineStep(feature_selection),\n",
    "    ],\n",
    ")\n",
    "\n",
    "transform_test_pipeline = Pipeline(\n",
    "    \"TRANSFORM_TEST\",\n",
    "    [\n",
    "        # Load data\n",
    "        PipelineStep(load_test_data),\n",
    "        PipelineStep(set_missings),\n",
    "        PipelineStep(reduce_mem_usage),\n",
    "        # New features\n",
    "        PipelineStep(generate_test_fe_feature),\n",
    "        PipelineStep(generate_test_dpi_feature),\n",
    "        PipelineStep(generate_test_bnum_feature),\n",
    "        # Feature selection\n",
    "        PipelineStep(remove_abon_id),\n",
    "        PipelineStep(feature_selection),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = transform_train_pipeline.run()\n",
    "\n",
    "print(\"Shape:\", train_data.shape)\n",
    "print(\"Columns:\", train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_key = \"cache/train_data_pipeline.pkl\"\n",
    "\n",
    "# with open(cache_key, \"wb\") as f:\n",
    "#     pickle.dump(train_data, f)\n",
    "\n",
    "with open(cache_key, \"rb\") as f:\n",
    "    train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = transform_test_pipeline.run()\n",
    "\n",
    "print(\"Shape:\", test_data.shape)\n",
    "print(\"Columns:\", test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_key = \"cache/test_data_pipeline.pkl\"\n",
    "\n",
    "with open(cache_key, \"wb\") as f:\n",
    "    pickle.dump(test_data, f)\n",
    "\n",
    "with open(cache_key, \"rb\") as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "def oversampling(train_data):\n",
    "    train_x = train_data.drop(columns=\"target\")\n",
    "    train_y = train_data.target\n",
    "\n",
    "    not_churn_data_count = train_data[train_data.target == 0].shape[0]\n",
    "\n",
    "    not_churn_count_strategy = int(not_churn_data_count * 0.6)\n",
    "    churn_count_strategy = int(not_churn_data_count * 0.7)\n",
    "\n",
    "    rus = RandomUnderSampler(random_state=SEED, sampling_strategy={0: not_churn_count_strategy})\n",
    "    train_x, train_y = rus.fit_resample(train_x, train_y)\n",
    "\n",
    "    smote = SMOTE(random_state=SEED, sampling_strategy={0: not_churn_count_strategy, 1: churn_count_strategy})\n",
    "    resampled_x, resampled_y = smote.fit_resample(train_x, train_y)\n",
    "\n",
    "    # resampled_x, val_x, resampled_y, val_y = train_test_split(resampled_x, resampled_y, test_size=0.1, random_state=SEED)\n",
    "\n",
    "    return resampled_x, resampled_y\n",
    "\n",
    "resampled_x, resampled_y = oversampling(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"random_state\": 42,\n",
    "    \"seed\": 42,\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"verbosity\": -1,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"feature_pre_filter\": False,\n",
    "    \"lambda_l1\": 2.916226764803635,\n",
    "    \"lambda_l2\": 5.574590549401839,\n",
    "    \"learning_rate\": 0.07518766667831955,\n",
    "    \"num_leaves\": 320,\n",
    "    \"feature_fraction\": 0.3888218905277871,\n",
    "    \"bagging_fraction\": 0.26715088946500626,\n",
    "    \"max_depth\": 17,\n",
    "}\n",
    "\n",
    "dtrain = lgb.Dataset(resampled_x, label=resampled_y)\n",
    "\n",
    "model = lgb.train(model_params, dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics\n",
      "AUC: 0.898\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.98      0.97    140597\n",
      "         1.0       0.59      0.42      0.49      9403\n",
      "\n",
      "    accuracy                           0.95    150000\n",
      "   macro avg       0.78      0.70      0.73    150000\n",
      "weighted avg       0.94      0.95      0.94    150000\n",
      "\n",
      "Columns count  612\n"
     ]
    }
   ],
   "source": [
    "X = test_data[resampled_x.columns]\n",
    "y_true = test_data.target\n",
    "\n",
    "y_pred_proba = model.predict(X)\n",
    "threshold = 0.5\n",
    "y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "Metrics().call(y_true, y_pred, y_pred_proba)\n",
    "\n",
    "print(\"Columns count \", len(resampled_x.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls = lgb.LGBMClassifier(**model_params)\n",
    "\n",
    "model_cls = model_cls.fit(resampled_x, resampled_y, init_model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics\n",
      "AUC: 0.900\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.98      0.97    140597\n",
      "         1.0       0.61      0.43      0.50      9403\n",
      "\n",
      "    accuracy                           0.95    150000\n",
      "   macro avg       0.78      0.70      0.74    150000\n",
      "weighted avg       0.94      0.95      0.94    150000\n",
      "\n",
      "Columns count  612\n"
     ]
    }
   ],
   "source": [
    "X = test_data[resampled_x.columns]\n",
    "y_true = test_data.target\n",
    "\n",
    "y_pred_proba = model_cls.booster_.predict(X)\n",
    "threshold = 0.5\n",
    "y_pred = (y_pred_proba > threshold).astype(int)\n",
    "\n",
    "Metrics().call(y_true, y_pred, y_pred_proba)\n",
    "\n",
    "print(\"Columns count \", len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save BEST_LightGbmV2_pipeline_p061_r043_090auc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'BEST_LightGbmV2_pipeline_p061_r043_090auc'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_final_model(model_cls, \"BEST_LightGbmV2_pipeline_p061_r043_090auc\", list(resampled_x.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
